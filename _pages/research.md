---
permalink: /research/
title: "Research"
layout: archive
author_profile: true

---

My research insterest lies broadly in stochastic approximation, MCMC, optimization, and machine learning.

- **Zero-Order Langevin Monte Carlo via SPSA under Noisy Function Measurements** 1/10-current
- Supervised by Prof. James Spall, Johns Hopkins University
  - <span style="font-weight:bold; text-decoration:underline;">H. Li</span>, J. Spall, manuscript in final polish
  - Motivation & method: Bridged gradient-based samplers and gradient-inaccessible settings in practice (simulation- based inference, model-free reinforcement learning, and stochastic control) by proposing Langevin Monte Carlo- Simultaneous Perturbation Stochastic Approximation (LMC-SPSA) in noisy settings-gradient-free Langevin sampler with only two noisy function queries per iteration (dimension-independent)
  - Convergence: Proved Wasserstein-2 convergence of LMC-SPSA under noise and resolved open step size question by deriving diminishing-step size window ensuring convergence, extending prior constant-step size analysis
  - Sharper dimension dependence: Improved the theoretical dominant dimension dependence of the Wasserstein-2 error bound under standard smoothness/noise conditions, with numerical results supporting the theory
  - Empirical results under equal budgets: LMC-SPSA attains the lowest, most stable MSE and moment errors (mean/variance/kurtosis), outperforming LMC-Finite Difference Stochastic Approximation and LMC-Random direction Stochastic Approximation


- **Fine-tuning FinBERT with Few-Shot Weak Supervision for Financial News Sentiment** 8/24-2/25
- Supervised by Prof. Helyette Geman, Johns Hopkins University
 
  - Motivation & method: Applied few-shot in-context labeling with gold examples to generate weak labels at scale, using consistency voting and confidence filtering for denoising; then fine-tuned FinBERT (LoRA) with temporal splits and probability calibration. The fine-tuned model outperformed the baseline FinBERT in accuracy
  - Multi-ticker disambiguation: Designed three datasets for multi-ticker articles (single-ticker, most-mentioned, and mention-share weighting); most-mentioned gave the highest accuracy and strongest 2-day Granger-prediction
  - Exploratory identification: Applied Difference-in-Differences around pre-specified shocks and Regression Discontinuity Design at set thresholds, uncovering potential causality between sentiment factors and stock returns


---

- **Causal Inference on High-Dimensional Time Series using LLM-Guided Discovery** 10/24-2/25
- Supervised by Prof. Helyette Geman, Johns Hopkins University

  - Developed novel framework leveraging causal order priors generated by LLMs to constrain discovery algorithms, enabling efficient learning of sparse structures and mitigating the curse of dimensionality
  - Designed and implemented soft constraints, conflict checks, and multi-round prompting, to minimize impact of spurious directions from imperfect LLM outputs, ensuring robustness and reliability of the inferred graphs


---
